---
title: "01_encode_portal"
date: "12/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The goal today will be to get familiar with TONS of GREAT
DATA on the ENCODE portal.

FIRST let's make a folder for everyone on fiji

```{BASH}

cd /scratch/Shares/rinnclass/CLASS_2022
  
mkdir "Your name -- or whatever you want"

```

Next let's go to the ENCODE portal and check out what is there.

For book keeping the final "query" is here:

```{HTML ENCODE DATA}

https://www.encodeproject.org/report/?type=Experiment&status=released&assay_slims=DNA+binding&biosample_ontology.term_name=HepG2&assay_title=TF+ChIP-seq&biosample_ontology.classification=cell+line&files.read_length=100&files.read_length=76&files.read_length=75&files.read_length=36&assay_title=Control+ChIP-seq&assay_title=Histone+ChIP-seq&files.run_type=single-ended

```

# Here is what we selected:

# "chromatin" interaction data, then DNA binding data, cell line HEPG2, "TF-Chip-seq". 

# We furhter selected "TF Chip seq", Control chip seq and histone chip seq. 

# We selected several read lengths to get the most DNA binding proteins (DBPs)
# Read lengths: 100, 76, 75, 36

# ONLY SINGLE END READS (this eliminates 54 samples)

We end up with a total of 554 biological samples, almost all have a biological replicate
and some have technical replicates, which we won't use. 

So now we need to make a sample sheet that connects the file id to the DBP and 
then use that for batch download from ENCODE.

# Step 1: download TSV with lots of information on each experiment

On the top of the site there is a "download TSV" click that and we can get started!
You now have a .TSV (tab seperated file or /t) 

# Step 2: download FASTQ files (raw sequence files)
Next we need to get the FASTQ link for each of the replicates.
CLick on the "List" button on the top left.
Click Download (select all files matching the criterian)

# Step 3: bring these two files into your directory
Use your file transfer system to move these files to your directory.

# Step 4: to download the data we need to use 'curl' or 'wget'

```{BASH}

man curl 
man wget

# These two commands will download the file(s) into your current directory.
```

# TEST download 

First delete the URLS so there are only two URLs to download.
or cut and paste two URLS into a new file called files.txt (using nano)

```{BASH}

man nano

# Basically a really easy to use text editor in BASH/unix

```

Creating a new files.txt with only 2 urls 

```{BASH}

nano files_test.txt

# paste these in

https://www.encodeproject.org/files/ENCFF212GYT/@@download/ENCFF212GYT.fastq.gz
https://www.encodeproject.org/files/ENCFF434BJG/@@download/ENCFF434BJG.fastq.gz

```
Let's read the file with cat.

```{BASH}

cat files_test.txt
# Do you see the URLS?

```

Since we have a file with multiple lines we want to envoke the shell to 
read all of the URLS and then pass all those onto the CURL command.
Let's take a look at xargs that can do this for us.

```{BASH}

man xargs

# basically xargs passes the current memory to a task and then moves to the next task
```

# This is the command encode suggested using to batch download files.

```{BASH}

xargs -L 1 curl -O -J -L < files_test.txt

```
This may look a bit scary at first but let's break it down.

# What is xargs thinking about?
-L is a argument for xargs termed "max lines" which is set to 1.
this means that xargs will read one line and move it to curl.
then xargs will repeat this for everyline in the "files_test.txt"
xargs is given the input of files_test.txt by the < input symbol ( > for output)

# What is curl thinking about?

Ultimately files.txt is put into the memory of the computer (xargs) and then sent to CURL.
Curl will then retrieve the data hosted by each URL. The flags in the curl command do the 
following.

# -O (output) 
by default unix keeps everything in "standard output" or "memory" then it wants you 
to tell it when to print that out etc. Standard output is a good term to remember in general.
Anywhoo we are using the -O flag to have curl print the file it is comminting
to standard output. So in short this ensures it will print the file after retreiving
it's contents and you can change the name of this output file.

# -J (replace string)
this makes sure after one URL is commited to standard input (the resulting file
to standard output) that it erases the previous standard input. If this was not
flagged then the next line would be appended to the previous and we would get
one monster compiled file of all the URLS!

# -L (location) 
if the URL has been changed to a new one it will be sent forward to the new
location

Ok now let's use this command to batch download data 
(note the files will download in directory you are currently in -- good to use 'pwd')

```{BASH}

xargs -L 1 curl -O -J -L < files_test.txt

# ****** If you just realized you are downloading a ton of data you can stop this process anytime with: ctrl-c ******

```
Sometimes if curl doesn't work wget will -- it's just good to have options

```{BASH}

wget -i files_test.txt

# here we see there is a flag for a "list" or -i
```

Same result? This seems so much better but if urls changed or were updated etc
we would want to move back to more of an arugment with xargs. 
In short there are so many ways to do the same thing!

# What if you were downloading a thousand files for a day or so? 
# Do you turn you power savings to never turn off?
# The bash solution: SCREEN

```{BASH}

man screen

```

# important syntax for screen session:
control A is for attach
D is for detach
screen -r #session# is to reattach (to session number below)
screen -list (tells you all the screen session you have runnign)

Ok, we now know how to access the WWW and download anything that is available!

#But how do we know if we downloaded the right file? Kinda scary right, what if the
file was missing a few lines or had some random internet glitch that made a gap
in the data? Yikes! 

# Bash solution: m5sum 

The original generator of this file will often provide a md5sum with the file you
want to download from them. This is a digital key that represents the exact nature
of the original file. md5sum is a command that can scan a file and produce this key
and if the files are identical md5sum logic will produce identical keys. Phew!

**** Please note how important this simple aspect is! What if you got new data from a
sequencing platform -- you typically download through and FTP site. Do you know
if your download was 100% successful? Not with out md5sum checks. Always request
an md5sum for any sequencing data you download *** 

```{BASH}

man md5sum

```

*** In fact the pre-run 17 of 1099 downloads failed to have the exact same file!

# Did we downloaded the right files? 
First let's get the md5sum values for the two URLS. Use "accession" in url 
to search encode portal and see what the md5sum is for the two files.
you may have another name for it on your computer such as md5 on macosx. but
on most servers it will be md5sum

# lets run md5sum to check if the files downloaded correctly.

```{BASH}

md5sum *.gz 

# check to see if your md5sum matches that on encode website. 

# note you can use md5sum on compressed files too such as .gz

```

# Would you want to do 1,099 times? 
Probably not so luckily we can check a list! let's make a list with nano

```{BASH}

nano md5sums.txt

```

The syntax for this is md5sum# "  TWO  SPACES  " and file.
For example:

```{BASH}

4b3e7dc77448bc4971367a3b40196cc0  ENCFF212GYT.fastq.gz
14fcf34bf1846ba82c4341838bc6e1b4  ENCFF434BJG.fastq.gz
```

We can go back to the encode portal and find md5sum (not in downloaded info)

Let's check if this data file is properly downloaded:
-Navigate to the data directory in CLASS_2022
-run the md5sum check:

```{BASH}

md5sum ENCFF212GYT.fastq.gz

# the result should be: 4b3e7dc77448bc4971367a3b40196cc0

md5sum ENCFF434BJG.fastq.gz

# the result should be: 14fcf34bf1846ba82c4341838bc6e1b4

```
# Note this approach is not very useful for 1,000+ fastq files being downloaded.

In the design file lecture we will use BASH and ENCODE API to
attach md5sums to each file automatically. Then download them all and check the
files. So don't worry we will get there in a way that doesn't involve cutting 
and pasting :) We are simply exploring the importance of these basic principles
of data science. 

Let's make a list of checksums and files. syntax again is md5sum -- two spaces -- file_name. In the data directory lets test the 3 fastq files are not corrupted.

```{BASH}

nano md5sums.txt

#paste in:

4b3e7dc77448bc4971367a3b40196cc0  ENCFF212GYT.fastq.gz
14fcf34bf1846ba82c4341838bc6e1b4  ENCFF434BJG.fastq.gz

```

Now we will use the -c flag to indicate we want to check a list of md5sums so synatx is:
md5sum -c file_name.txt

Let's run it:

```{BASH}

md5sum -c md5sums.txt > md5sums_status.txt

```

Congratulations -- all of ENCODE is now available ! Next we will continue
practicing BASH/unix in the .TSV file we downloaded earlier. Please don't download the 1000+ URLS we got from encode today :)


*********************
EXCERCISE
*********************
Write a shell script that will download 3 fastq files from encode portal and check the md5sums.
Some hints:

A shell script is a list of commands you submit to fiji using the SLURM language. So we will send the
curl or wget to get the files from encode as you did above. Then have fiji check the md5sums after they download. A shell script is just a set of commands in one file ending in .sh (you can make with nano)

This requires a shebang on the first line:
#!/bin/sh
Command 1
command 2 etc...

Once you have your commands in the .sh file you need to make it executable:

```{BASH}

chmod u+x filename.sh

# note this needs to be done each time a slurm job is submitted
```

To run your shell script you will use 

```{BASH}

./filename.sh

```

Thats it go have fun !


Go select 3 data sets of your interest in the encode portal (any cell type etc etc)
You want to end up with three fastq.gz files (hint use screen session and)
Then run the md5sum ()

