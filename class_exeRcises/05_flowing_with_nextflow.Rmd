---
title: "Nextflow"
author: "JR"
date: "9/29/2021"
output: html_document
---

Today we will install nextflow and install the nf-core/chip-seq pipeline. Briefly, Nextflow
is a taks manager that will send thousands of tasks (some in parallel) to fiji. NF_CORE has
developed pipelines for ChIPseq, RNAseq, ATACseq and pretty much anything with seq. We need 
to install Nextflow on fiji so they can communicate. Ultimately we will use nextflow to task
manage the NF_CORE pipeline that will run ChiP seq from genome alignment to calling peaks to
providing all the QC needed for publication. 

Nextflow has amazing documentation and a good place to start is here:
https://www.nextflow.io/docs/latest/getstarted.html


Let's install Nextflow into our bin directory
 

****************
Step 1 Install:
****************

Nextflow will install itself with this simple commmand below. However, you will
want to think about where to install it. 

A reasonable place to install it is in a "bin" directory within your home directory

/Users/<identikey>/bin/nextflow

Alrighty, give it a go:

```{bash}
cd ~
ls -lah
cd bin

# if you don't see a bin directory make one using the commands below. 

mkdir bin
cd bin

# Then install nextflow
curl -s https://get.nextflow.io | bash
```

Nice it's installed just like that! curl went and got a bash script (silent -s)
and the results of that query were piped to bash which runs the script to install itself.

This is nifty, but the fact that you can run a script from the internet highlights that you
should be careful that you trust the website, etc.

If you ever want to update nextflow just run the above. At the time of this doc
it is at: v20.10.0

********************************
Step 2: Add nextflow to $PATH
********************************

The $PATH is a default place the computer will look for commands. Imagine you
had to tell Bash where the ls command is?

/usr/bin/ls to list files instead of just ls

So that path is an important aspect of unix/bash that you will never really hear
about until it becomes a bug :) 

Let's put nextflow in our path (just as ls, cd etc are) so we can call it from anywhere on fiji.

```{bash}

echo $PATH
# we can make it easier to read with TRANSFORM (tr) a powerful bash command to
# repalce and find text.

echo $PATH | tr ":" "\n"
# kinda hard to read with : seperated file
```


Now let's add it to our .profile file which will load each time you login to the terminal
```{R}
cd ~/.profile
```

To append a directory to PATH, we just need to overwrite the path variable
To assign a variable in bash the assignment operator is the equals sign. 
When referring to the contents saved in an environment variable, the $ is used
```{bash}

PATH=$PATH:~/bin
```

This will change the PATH environment variable during this session only,
but now we want this in the profile that is loaded at each login 

```{bash}
nano .profile
PATH=$PATH:~/bin
source ~/.profile
```


Now check that nextflow can be run without referring to it's location
```{BASH}

nextflow -version
```

Sweet, that's it. We've successfully installed nextflow, added to $APTH and can use from anywhere now!


****************
Step 3: Install nf-core/chipseq pipeline
****************

Now we want to install the chipseq pipeline from NF-core. The cool thing about 
this set up is essentially you are going to clone the latest github for chipseq
by default. Simply run:


```
nextflow pull nf-core/chipseq
```

If you want a specific version you can use the flag to call that version.
At the time of this document chipseq is at 1.2.1. 

```
nextflow pull -r 1.2.1 nf-core/chipseq
```

Whenever the pipeline is run, it actually does this step behind the scenes,
so you don't actually need to do this in order to have the chipseq pipeline install.
This means that if you want to run an older version, you can just indicate that when
you're running that pipeline and it will install that version if it's not 
already installed.

*********************
Step 4: Think about your directory structure
*********************

We have set up the class with the following directory structure -- let's make your directory nice and tidy.
We will run our first test ChIP in the work directory.

```{BASH}
cd /scratch/Shares/rinnclass/your_folder.
mkdir results
mkdir work
mkdir analysis

cd work
```



*********************
Step 4: Input files.
*********************

The NF_CORE ChIPseq pipeline requires a few input files.

1) Config file:  tells next flow how to talk to Fiji
2) shell script: info on what resources can be used on Fiji & where input files are for pipeline
3) 




Now we need to tell nextflow how to run on fiji by giving some instructions via config_file.

executor : what language does Fiji speak (Next flow speaks many languages like C3PO)
SLURM is a simple job scheduler.

queue : there is a long and short queue (<24h)

memory : This sets RAM or memory SLURM can use to communicate with next flow (usual doesn't need much)

maxforks : this tells nextflow how many processes can be run in parallel. Let's 
say you had 10 fastq files to run through a pipeline you can speed up the process
of running 10 tasks in parallel!




```{bash}
cd /scratch/Shares/rinnclass/<name>/
mkdir first_chipseq
cd first_chipseq

# Now let's create a nextflow.config file
nano nextflow.config
# Then paste in what's below
```


```{bash}
process {
  executor='slurm'
  queue='short'
  memory='32 GB'
  maxForks=10
}
```

Above is a typical nextflow.config file that is needed to run nextflow on fiji.

Note that the sbatch set up used less RAM etc. That is because the sbatch is 
"controling the job flow" and doesn't need as much juice to do send and manage
jobs. However, nextflow is calling on the server seperately for it's memory usage.
This means nextflow will be running with more juice ...


********************************
Step 5: SLURM - run.sh
********************************

We could run nextflow directly from the command line, but since the total
time to compute for the nextflow chipseq pipeline can be hours to days,
and the job will stop running when you exit your terminal, we need a different 
solution. We could use screen, but that will result in the nextflow process running
on the login-node. Instead we should submit a job to slurm that will
run the nextflow process and orchestrate the execution of jobs.


The chunk below is an example of how we will invoke nextflow using a slurm job on fiji.


Essentiall the run.sh file is everythign we need to set up all the details of the run.  These are slurm instructions and nextflow learned it would be in slurm via the config file. We will use SBATCH to submit batch instructions to slurm. 

In otherwords: When the job allocation is finally granted for the sbatch script,
the nextflow process will start running on one of the compute nodes.

First let's look at the sbatch / slurm communication set up. Each aspect is 
described with a # below.

```{BASH}

#!/bin/bash
#SBATCH -p short
#SBATCH --job-name=Hepg2_Pol_test
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=john.rinn@colorado.edu


### These lines are telling the slurm scheduler what resources we need. Here we're saying we
### want to be in the short queue (For jobs that take less than 24 hrs to run. Name of the job, who to email etc...

#SBATCH --nodes=1
### Fiji has multiple nodes and you will rarely want to run a job across more 
### than one node. 

#SBATCH --ntasks=1
#### This is how many cpus you're requesting. We wil typically
#### leave this at 1 so the nextflow runs only on one cpu (single-threaded). 
#### For some software, they can by run multicore and you may want more that one cpu.


#SBATCH --mem=6gb

#### Simply setting how much RAM memory you're requesting for your job. This is an important consideration
#### depending on how much RAM your software uses to do the compute task.
  

#SBATCH --time=10:00:00

## The other noteable is the "WALL CLOCK" 
## THIS IS WHERE GOOD MANNERS come into play. You want to test this out on a few
## files and then think about how it will scale. This run.sh file is for thousadnds
## of FASTQ files, we will change the wall clock.


#### We also want to follow standard error and output. This will allow us to track
#### the progress of the run. 


#SBATCH --output=nextflow.out
#SBATCH --error=nextflow.err

```

So above SBATCH organized all the instructions needed for slurm to scehdule the job.
However, now we need set the file paths for next flow. We are using nextflow
command language that is in the documentation. 


---- Setting the nextflow parameter and file paths ----

Now that we have SBATCH all set up we need to set the nextflow parameters for the
job. 


Here is the second half of the run.sh file needed below. The description of each
follows after the chunk.

```{bash}
# Make a run.sh
nano run.sh
# Paste in what's below
```


```{BASH}
#!/bin/bash
#SBATCH -p short
#SBATCH --job-name=Hepg2_Pol_test
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=john.rinn@colorado.edu

pwd; hostname; date
echo "Lets do chipseq"

module load singularity/3.1.1

nextflow run nf-core/chipseq -r 1.2.1 \
-profile singularity \
--single_end \
--input design.csv \
--fasta /Shares/rinn_class/data/genomes/human/gencode/v32/GRCh38.p13.genome.fa \
--gtf /Shares/rinn_class/data/genomes/human/gencode/v32/gencode.v32.annotation.gtf \
--macs_gsize 3.2e9 \
--blacklist /scratch/Shares/rinnclass/data/hg38-blacklist.v2.bed \
--email john.rinn@colorado.edu \
-resume \
-c nextflow.config

date



### add the sbatch information to this above and make run.sh file in your directory
```

Each line is a command ended by \

The backslash is a way to write multi-line bash commands that will be interpreted by the shell as one line.

The first step is telling the pathway, hostname and date of where and when job was run.
the echo command is sort of silly and just let's you know SBATCH was successful!

module load singularity/3.1.1 is going to load singularity and tell bash that it
is using singularity:

Singularity: This is telling nextflow that instead of installing all the software locally,
it should go and retrieve the software from a singularity container that contains everything
needed to run the pipeline. 

Profile: this has a single flag is for an abbreviation of a command. Double flags
are specifying directly to the command name. This tells the
server to use the singularity container for instructions on running the pipeline
that are set up by nextflow commands.

--Single end: the defualt is paired end for nextflow, if the data is single end
read then we need to tell nextflow that.

-- input: is the design file so next flow knows which are controls, replicates etc
more on this below.

-- fasta: one of the initial steps in the nextflow pipelien is alining the reads to
the genome of interest. So to make sure that is precise and reporducible you 
provide the file path to the genome (in our case it is preplaced in file path).

-- gtf: this is the file of annotations in the human genome. Here we will use 
Genocode. Gencode's website provides a GTF of whatever release you prefer here:

https://www.gencodegenes.org/human/

-- macs_gsize: this is the effective genome size that is required by the MACS 
peak calling algorithm in the nextflow pipeline. The genome is about 3 billion
bases is what we are saying here. 

-- blacklist: the reason we didn't 3B above is that many regions of the genome are not good
to align to for various reasons (low complexity etc). This is telling nextflow what
regions of your genome are bad and it will stay away from these coordinates.

-- email so nextflow can email you status on your job (SLURM will too :)

-- resume: is nextflow command that if there is a failure you don't have to start
at the beginign fo the pipeline each time.

-c abrreviation for config file.

date: when the pipeline finishes it will print the bash date command (we used above)


*******************************************
Step 6:Running and Checking on slurm jobs
*******************************************

First thing we want to do is "run" a .sh file. Since we set this up in SBATCH we
simply type:

Don't run yet though
```
sbatch run.sh
```
This will have sbatch send the scripts in the run.sh file to slurm to run them. 


--- what jobs are you running? You often want to see what job is running how long it is taking and other aspects. To that end we can use 'squeue' to see what is running

```
squeue -u identikey
```

the -u flag is for user.
examine the information provided and google the column headers for more info.

---- canceling jobs


Let's say you accidently hit the run button and want to cancel immediately.

'scancel' is the command

```
squeue -u identikey
scancel jobid
scancel -u identikey #cancels all running jobs
```

--- canceling multiple jobs:

```
squeue -u identikey | grep commonjobid# | awk '{print $1}' | xargs -n 1 scancel

squeue -u jori2700  | grep 591 | awk '{print $1}' | xargs -n 1 scancel
```

Here we are using some bash to grab all the jobs running. They will typically have a common number (591 in example above -- would have aloso worked with just 5).

Grep finds all your job id matches, then awk prints them, xargs is then going 
to perform scancel on the output of awk, the -n is telling xargs how many arguments will follow. In this case just 1 -- scancel.

Voila your jobs are gone :)


**********************
Step 7: Design file
**********************

Nextflow documentation has a clear set up for the Chipseq pipeline. 
You must have the folowing columns (seperated by ',')

```
group,replicate,fastq_1,fastq_2,antibody,control
```


GROUP: the name of the gene targeted or sample information (e.g. POL2)

REPLICATE: if you have replicates you number them 1,2 .... in each row. If you 
have only one, you need to type in 1.

fastq_1: file path to the fastq files you downloaded from ENCODE.
fastq_2: if you have dual end reads the second read fastq file path.

ANTIBODY: this is the target and same a group name for non-control samples.

CONTROL: in our case this is input DNA from ChIP. ENCODE let us know what the 
control accession is. 

NOTE: the controls still need to be places as a 'group' entry. Howevever, their
'control' column is empty.


********************************
Step 8: Test run of design file!
********************************

You will need these files -- and change the file paths in the design file below
to where the fastq files are for you. You can download them with these links and wget:

https://www.encodeproject.org/files/ENCFF210PXS/@@download/NCFF210PXS.fastq.gz
https://www.encodeproject.org/files/ENCFF525AYL/@@download/ENCFF525AYL.fastq.gz
https://www.encodeproject.org/files/ENCFF162ADN/@@download/ENCFF162ADNP.fastq.gz



Here is a mini design file -- you will have to change the file path to the fastq
files accordingly. 

I am putting then design below as a tab serpated file. Can you use transmute 
to put it back to comma seperated?

```
group	replicate	fastq_1	fastq_2	antibody	control
ASH2L	1	../data/ENCFF210PXS.fastq.gz		ASH2L	ENCSR055XHN
ASH2L	2	../fastq/ENCFF525AYL.fastq.gz		ASH2L	ENCSR055XHN
ENCSR055XHN	1	../fastq/ENCFF162ADN.fastq.gz
```


Hint:

```
cat design.csv | tr '\t' ',' > desgin2.csv
```


group,replicate,fastq_1,fastq_2,antibody,control
ASH2L,1,../fastq/ENCFF210PXS.fastq.gz,,ASH2L,ENCSR055XHN
ASH2L,2,../fastq/ENCFF525AYL.fastq.gz,,ASH2L,ENCSR055XHN
ENCSR055XHN,1,../fastq/ENCFF162ADN.fastq.gz,,,

NOTE: remember to make sure the file path is correct to the fastq on your computer.
here i am saying the fastq directory is one directory above then the path to the
file name.

Once you have all set:

```
sbatch run.sh
tail -f nextflow.out
```

This will run the pipeline and tail -f allows you to follow the end of the nextflow
output as it is going.

Go check out all the results in the results folder created by nextflow! In the 
next class we will go over all the output files and how to visualize the data
produced (bigWig and peak_files)

Excercise: Make a design file for a diferent DNA binding protein in the encode
dowload list. We can debug all the problems in class.






## Homework Fun ##

To start Run each of these lines in a row. 
```{BASH}

mkdir time_script
cd time_script

cat >> what-time-is-it.sh << 'EOF'
#!/bin/bash

current_time=$(date | tr -s " " "\t" | cut -f 4 | cut -d ":" -f 1,2)
#notice use of transmute (tr) and cut -- these are just grabbing the information
#bash date command output looks like: Sun Nov 15 18:06:10 MST 2020. Hint cut -f 4
# grabs the 4th item which is the time.

# Exercise see how transmute and cut grab the information more succinctly.

echo "The time is $current_time.
I'm glad to see you're making good use of it :)"

# this is just adding silly text to output with the time.


# EOF is End of File we set on the first line.
EOF

chmod +x what-time-is-it.sh

# chmod changes the permissions to a file, the +x gives it "executable" permission
# to go from shell to Kernel.

ls
cat what-time-is-it.sh

## Notice it wrote out the file what-time-is-it with the "cat >>" command.
```

Ok so we have a time telling script that only works in the current directory:

This is where I made it:

/Users/jori2700/CLASS/time_script

So now we can add this to $PATH and run the script whenever just like ls, cat etc.

First we are going to add the .sh script to our $PATH temporarily.

```{BASH}

pwd
#copy and paste path to the directoy the .sh script is in


export PATH="$PATH:/Users/Users/jori2700/CLASS/time_script"
what_time_is_it.sh #(notice we don't need the ./ now that the path is set)

```


Now how to permenantly add to you $PATH

```{BASH}

echo 'export PATH="$PATH:/Users/jori2700/CLASS/time_script/"' >> ~/.bash_profile
# nice let's take a look
echo $PATH | tr ":" "\n"
# we need to activate our new $PATH with
source ~/.bash_profile

cd anywhere
sh what-time-is-it.sh
```

Voila we can use the script anytime. Also this is a nice idea to have new scripts
etc in this folder as they will automatically be in $PATH now. To run a .sh file
you want to type "sh filename" in this case sh waht-time-is-it.


