---
title: "17_RNase_Part_I"
author: "JR"
date: "11/10/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(stringsAsFactors = FALSE)
library(tidyverse)
library(httr)
library(janitor)
library(purrr)


```

Goal: to downlaod a RNAseq data from HEPG2 that is fractionated by cellular compartment (nuc, cyto, total etc).
Along the way we will want to make a well orgnanized sample sheet to use later for DeSEQ.

So we are going to combine our skills from 02_downloading data and all the tidyverse functions :)

Here is a snap shot of the data to be downloaded -- let's start by putting in browser and taking a look


[Encode Query]("https://www.encodeproject.org/search/?type=Experiment&status=released&assay_slims=Transcription&assay_slims=Transcription&replicates.library.biosample.donor.organism.scientific_name=Homo+sapiens&biosample_ontology.classification=cell+line&files.read_length=50&limit=all&advancedQuery=date_released:%5B2009-01-01+TO+2021-12-31%5D&biosample_ontology.term_name=HepG2&assay_title=total+RNA-seq&biosample_ontology.classification=cell%20line")


Nice, we can get all the data we need. First we need to think about where to put this :)
# PLEASE don't download until class -- we will just download once in one place (big data)
To be safe let's make an rnaseq directory somewhere on scratch that we can all access.
Then download everythign we need!

```{bash downloading file list and sample sheet}
# You'll want to change directory to the rnaseq directory 
# this will get you a list of samples as a .txt file using wget.
# We will use this to make the sample sheet.

wget -O samples.txt "https://www.encodeproject.org/report.tsv?type=Experiment&status=released&assay_slims=Transcription&assay_slims=Transcription&replicates.library.biosample.donor.organism.scientific_name=Homo+sapiens&biosample_ontology.term_name=HepG2&biosample_ontology.classification=cell+line&assay_title=total+RNA-seq&files.read_length=50&limit=all&advancedQuery=date_released:[2009-01-01%20TO%202021-12-31]"

# This will give us a text file of the file names. We will use this to download the files.

wget -O files.txt "https://www.encodeproject.org/batch_download/?type=Experiment&status=released&assay_slims=Transcription&assay_slims=Transcription&replicates.library.biosample.donor.organism.scientific_name=Homo+sapiens&biosample_ontology.term_name=HepG2&biosample_ontology.classification=cell+line&assay_title=total+RNA-seq&files.read_length=50&limit=all&advancedQuery=date_released:[2009-01-01%20TO%202021-12-31]"

# Now let's make a common directory called HEPG2_rnaseq so we only download this once :)
# Download the fastq files -- ~50 GB 
xargs -L 1 curl -O -J -L < files.txt

# Cool we have all the data we need for the NF_CORE RNAseq pipeline in 18_

```



 ---- Introduction to APIs (Application Programming Interface) ----
In order to exchange information between someone's database and your computer you use an API -- basically a highly specified language for interacting and retrieving the data you need.


Today, we will use ENCODE's API to retrieve additional file information from their server.
```{r examining encode API}

# ENCODE base url: https://www.encodeproject.org/report.tsv?

# Let's look at an example request for this experiment accession: ENCSR541TIG
request_url <- "https://www.encodeproject.org/report.tsv?type=File&status=released&file_format=fastq&dataset=%2Fexperiments%2FENCSR541TIG%2F&field=accession&field=read_count&field=md5sum&field=controlled_by&field=paired_end&field=paired_with&field=replicate&field=target"

# the field parameter is where we tell it which columns or which pieces of data we want to get.
# this retrieves read_count, md5sum, controlled_by, paired_end, paired_with, replicate, and target

```

We've written some custom helper functions specific to the ENCODE API to request exactly the information we want, since we'll make these requests multiple times -- for each experiment accession.

Here are those functions. Read through and see if you can figure out what each step is doing. The ENCODE API provides extensive documentation as to what data you can request and how to format that data to make your request. You can browse the possible requests in an interactive way using their interactive documentation: https://app.swaggerhub.com/apis-docs/encodeproject/api/basic_search


We will make two functions:
1) construct_query
2) encode file info
```{r}

## This will generate a request URL in the format that ENCODE requires to retrieve each of the columns listed in the field default parameter (accession, read_count, md5sum, etc.)

# first let's set up the structure
contstruct_query <- function(experiment_accession,
                             base_url = "https://www.encodeproject.org/report.tsv?",
                             file_format = "fastq",
                             type = "File",
                             status = "released",
                             fields = c("accession", "read_count", "md5sum",
                                        "controlled_by", "paired_end",
                                        "paired_with", "replicate", "target")) {
  
  # Now we will populate this structure above, note experiment_accession is only
  # parameter we need to populate
  query <- paste(list(paste0("type=", type),
                      paste0("status=", status),
                      paste0("file_format=", file_format),
                      paste0("dataset=%2Fexperiments%2F", experiment_accession, "%2F"),
                      # map is a way of transforming input and applying a function
                      # in this case we are just using "paste0" as the function
                      # map_chr is to make sure it stays as a character value
                      map_chr(fields, ~paste0("field=", .))) %>%
                   flatten(),
                 collapse = "&")
  url <- paste0(base_url, query)
  return(url)
}

# This function actually makes the request and returns the data only (without the response headers) in a data.frame format.

encode_file_info <- function(experiment_accession,
                             base_url = "https://www.encodeproject.org/report.tsv?",
                             file_format = "fastq",
                             type = "File",
                             status = "released",
                             fields = c("accession", "read_count", "md5sum",
                                        "controlled_by", "paired_end",
                                      "paired_with", "replicate", "target")) {
  
  # Now we are creating a url that encode will understand
  path <- "report.tsv?"
  base_url <- modify_url("https://www.encodeproject.org/", path = path)
  url <- contstruct_query(experiment_accession,
                          base_url = base_url,
                          file_format,
                          type,
                          status,
                          fields)
  # this is now retrieving the data with GET function in httr
  resp <- GET(url)
  if (http_error(resp)) {
    error_message <- content(resp, type = "text/html", encoding = "UTF-8") %>%
      xml_find_all("//p") %>%
      xml_text() %>%
      first()
    stop(
      sprintf(
        "ENCODE API request failed [%s]\n%s",
        status_code(resp),
        error_message
      ),
      call. = FALSE
    )
  }
  
  if (http_type(resp) != "text/tsv") {
    stop("API did not return text/tsv", call. = FALSE)
  }
  body <- read_tsv(content(resp, "text"), skip = 1) %>%
    clean_names()
  return(body)
}
```

We can now test that this function delivers what we want it to using the same accession we used previously.


Note that since all of the parameters except the accession number have default values.
If we want the defaults, we only need to provide the ENCODE accession of the experiment we want.


```{r}
dat <- encode_file_info("ENCSR541TIG")

# Nice we just retrieved all the information we wanted for this experiment accession!
```

## Introduction to map

There is one other function we need to learn about before we make the queries to the ENCODE API.
Specifically the map function from the purrr package.
This allows us to query each experiment accession and load the data we retrieve into the same data.frame that contains the sample info.

It's very similar to sapply or lapply, but is more consisten with tidyverse syntax and allows us to do lapply inside a data.frame & with pipe syntax.

One other way to think about map is the information we just got from:
dat <- encode_file_info("ENCSR541TIG")
is places on "cell" of an excel file. So each cell contains an excel sheet in the cell.


```{r}
# In this example we'll take the digits 1 through 10 and "map" each to the rnorm function. 

map <- map_example <- 1:10 %>%
  map(rnorm, n = 10) 

# It returns a nested list, let's look:
summary(map_example)
table(summary(map_example))

# let's index and look
map_example[[1]]
map_example[[2]]

# if we want a specific index of a specifc rnorm array:
map_example[[1]][[1]]

```

We can use map to call this encode_file_info function for each experiment accession and return each data.frame into a new column in our samples data.frame.

First let's read in the samples we downloaded from ENCODE.

```{r reading in sample sheet}
# We'll also rename this Accession column to clarify between experiment_accession and file_accession.
samples <- read.table("samples.txt",
                      sep = "\t", skip = 1, header = T) %>%
  dplyr::rename(experiment_accession = Accession) 

# It's seems mundane but starting here is the best way to make a "reproducible" sample sheet.
# Bottom line: the download to code to analysis is the way to reproducibility (this worked a year later :)
```

```{r}

#TODO does map make a new column or is it mapping to experiment_acession from the encode_file_info function?
samples <- samples %>%
  mutate(file_info = map(experiment_accession, ~ encode_file_info(.x)))

# This short bit of code is doing a lot!
# we make a new column 'file_info'
# each entry will be the information gained from the encode_file_info function
```

This is a bit hard to read in this format, so we can unnest the data.frames in the file_info column using the unnest command.
This will duplicate the remaining columns to match the number of lines in the nested data.frame.

```{r}
# We also need to tell it which column we want to unnest.
samples <- samples %>%
  unnest(cols = file_info)

# Amazing now we have all the information for each file in all the experiment accessions
# we will use this to start making a sample_sheet.
```

Now we have all the information we neeed in this data.frame and we just need to clean it up.

```{r}
# Let's number our replicates and create a new column called sample id where we join the experiment accesion and the rep number
samples <- samples %>%
  group_by(experiment_accession) %>%
  mutate(rep_number = as.numeric(factor(replicate))) %>%
  unite(sample_id, experiment_accession, rep_number, sep = "_rep")

 # unite will make a new column in samples (piped in)
  # here we will combine experiment accession and rep number with _rep in between
  # This is a handy handle for the data to access later
```

Now let's get rid of all that data we don't need! 
```{r}

# We're just selecting a subset of columns from samples
samples <- samples %>%
   dplyr::select(sample_id, experiment_accession, Assay.title,
                Biosample.summary, md5sum, paired_end_identifier) %>%
  # Note that we're not removing the original columns
  unite(fastq_file, sample_id, paired_end_identifier, sep = "_read", remove = F)
```

Now let's make the full filename for the fastq files. For the nf-core/rnaseq pipeline, the paired-end reads need to be named with the read number in the filename. 

```{r}
samples <- samples %>%
  mutate(fq_extension = ".fastq.gz") %>%
  unite(fastq_file, fastq_file, fq_extension, sep = "", remove = F) %>%
  # This original file column will be used along with the new name column to rename the fastq files.
  unite(original_file, accession, fq_extension, sep = "")
```

We broke this down into parts, so you can understand what is happening, but just note that you can write all of this in one block and read it like a sentence.

```{r}
samples <- read.table("samples.txt",
                      sep = "\t", skip = 1, header = T) %>%
  dplyr::rename(experiment_accession = Accession) %>%
  mutate(file_info = map(experiment_accession, ~ encode_file_info(.x))) %>%
  unnest(file_info) %>% 
  group_by(experiment_accession) %>%
  mutate(rep_number = as.numeric(factor(replicate))) %>%
  unite(sample_id, experiment_accession, rep_number, sep = "_rep") %>%
  dplyr::select(sample_id, accession, Assay.title,
                Biosample.summary, md5sum, paired_end_identifier) %>%
  unite(fastq_file, sample_id, paired_end_identifier, sep = "_read", remove = F) %>%
  mutate(fq_extension = ".fastq.gz") %>%
  unite(fastq_file, fastq_file, fq_extension, sep = "", remove = F) %>%
  unite(original_file, accession, fq_extension, sep = "")
```

This cleaned up version of the samplesheet is good to go!
Now we want to rename the fastq files to the fastq name we just made.
```{r}
# Rename the fastq files so that they contain the sample ID.
rename_script <- samples %>%
  dplyr::select(fastq_file, original_file) %>%
  mutate(command = "mv") %>%
  unite(command, command, original_file, fastq_file, sep = " ")
# The result of this is that each row is a bash command.

# We can write this out as a bash script with ?write_lines 
# We include a shebang header line so that the script is interpreted by bash.

write_lines(c("#!/bin/bash", rename_script$command), "rnaseq/fastq/rename.sh")
# Here we use an R command to call bash and cd into that directory, then make the file executable, and then run it.
# system("cd rnaseq/fastq; chmod u+x rename.sh; ./rename.sh")
```

Additionally from all of this information we've gathered we can create a text file to run the md5sum check.

```{r}
# Let's create an md5.txt to run the checksums
md5 <- samples %>% 
  dplyr::select(md5sum, fastq_file) %>%
  unite(line, md5sum, fastq_file, sep = "  ")
write_lines(md5$line, "rnaseq/fastq/md5.txt")
# Now let's run it.
# system("cd rnaseq/fastq; md5sum -c md5.txt")
```

Finally, we can write out a nicely formatted sample sheet that we will use downstream for further analysis of the read counts in R.

```{r}
# Let's create the sample sheet that we will use later
# to do the RNA-seq analysis in R.
samples <- samples %>%
  dplyr::rename(fastq = fastq_file,
                seq_type = Assay.title,
                sample_name = Biosample.summary) %>%
  # The minus sign will remove this column -- which we no longer need.
  dplyr::select(-original_file) 

```


Now that we have it cleaned up, let's create one line for each replicate where the fastq read 1 and read 2 are in the same row.
```{R}

# 
# For this we will use the pivot wider function
# We need to tell the pivot_wider function which unique column combinations will specify each new row. 
samplesheet <- samples
samplesheet <- samplesheet %>%
  #id_cols is a parameter in pivot wider to select the cols
  pivot_wider(id_cols = c("sample_id", "seq_type", "sample_name"),
              names_from = paired_end_identifier,
              values_from = c("fastq", "md5sum"))


# Harmonize the sample names with the design file.
samplesheet <- samplesheet %>%
  mutate(condition = gsub(" ", "_", sample_name) %>% tolower()) %>%
  separate(sample_id, into = c("experiment_accession", "replicate"), 
           remove = FALSE, sep = "_") %>%
  mutate(replicate = gsub("rep", "R", replicate)) %>%
  unite(sample_name, condition, replicate, sep = "_", remove = FALSE)

# here we are just changing the hepg2 total name to hepg2_total
samplesheet$condition[samplesheet$condition == "hepg2"] <- "hepg2_total"
samplesheet <- samplesheet %>%
  mutate(cell_type = "hepg2",
         condition = gsub("hepg2_", "", condition)) %>%
  dplyr::select(sample_id, sample_name, replicate, condition,
                cell_type, seq_type, fastq_1, fastq_2, md5sum_1,
                md5sum_2)

write_csv(samplesheet, "/Shares/rinn_class/data/CLASS_2022/class_exeRcises/analysis/17_RNaseq_part_I/samplesheet.csv")
```

We also need to make a design file for the nf-core/rnaseq

The design file needs to be in a specific format (as we saw with nf-core/chipseq)
It needs the following columns:


sample,fastq_1,fastq_2,strandedness
Let's create a sample column using mutate -- and we'll clean up the names
```{r}

# There's spaces in the names currently and we need to get rid of those.
design <- samplesheet %>%
  # We also need to add the proper file path
  mutate(sample = gsub(" ", "_", sample_name) %>% tolower(),
         strandedness = "unstranded",
         fastq_1 = paste0("/Shares/rinn_class/data/CLASS_2022/data/rnaseq/rnaseq_fastq/", fastq_1),
         fastq_2 = paste0("/Shares/rinn_class/data/CLASS_2022/data/rnaseq/rnaseq_fastq/", fastq_2)) %>%
  # We have the replicate number already, but it's just a part of the sample id
  # We can use the separate function (opposite of unite) to retrieve it.
  separate(sample_id, into = c("experiment_accession", "replicate"), sep = "_", remove = FALSE) %>%
  mutate(replicate = gsub("rep", "", replicate)) %>%
  # Now we gather just the columns we need
  dplyr::select(sample, replicate, fastq_1, fastq_2, strandedness)

design <- design %>%
  mutate(sample = gsub("_r1", "", sample),
         sample = gsub("_r2", "", sample))

design <- design %>% 
  arrange(sample, replicate)

all(sapply(design$fastq_1, file.exists))
all(sapply(design$fastq_2, file.exists))
# Now we can write this out for input into nextflow

write_csv(design, "design.csv")

write_csv(samples, "samples.csv")

write_csv(samplesheet, "samplesheet.csv")
```


Now we have the raw files downloaded and the samplesheet needed for downstream analyses (ex, differential expression). So the next step is to run nf-core/rnaseq on these samples.
We'll do this in the RNAseq II.

